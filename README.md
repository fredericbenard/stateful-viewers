---
title: Stateful Viewers
emoji: üñºÔ∏è
colorFrom: blue
colorTo: indigo
sdk: docker
app_port: 7860
---

# Stateful Viewers

*A viewer whose perception evolves with each image*

Stateful Viewers is an art and research project that simulates a visitor walking through a gallery. A vision-language model reflects on images one at a time, carrying forward memory, attention, and affect so that each encounter subtly shapes how subsequent images are perceived.

Rather than treating images as independent inputs, the system models viewing as a continuous, cumulative experience.

### Architecture

![Stateful Viewers architecture](docs/architecture.png)

Each viewer has a **Profile** (perceptual dispositions), a **Reflection Style** (expressive voice), and an evolving **Internal State** (momentary inner condition). For each image, all three ‚Äî plus the image itself ‚Äî feed into a vision-language model, which produces a **Reflection** and an **Updated State** that carries forward to the next encounter.

## Features

- **Viewer Profile Generation (v2)** -- Generate a viewer profile, independent reflection style, initial internal state, short descriptions for each, and a label -- all in a single flow. Variability is ensured by parametric hints that randomly pin 2-4 of 7 dimensions per schema and let the LLM resolve the rest.
- **Public & Saved Profiles** -- Load pre-generated public profiles (EN + FR, under `data/profiles/public/`) or your own saved profiles. The profile list is not filtered by LLM provider -- you can mix a profile generated by one LLM with reflections from another.
- **English + French (EN/FR)** -- UI localization, locale-aware scraping (gallery names/sections/descriptions/captions), and locale-aware generation/TTS
- **Multiple Model Providers** -- Use Ollama locally (LLaVA-1.6 7B for vision + Llama 3.1 8B Instruct for text) or cloud providers (OpenAI GPT-5.2, Google Gemini 3 Pro (preview), Anthropic Claude Sonnet 4.5)
- **Stateful Reflections** -- Each reflection is conditioned by the viewer profile, reflection style, and an evolving internal state. On the first image the system uses the generated initial state; subsequent images carry forward the state from the previous reflection.
- **Structured Output** -- Reflections use `[REFLECTION]` and `[STATE]` blocks for clear parsing
- **Text-to-Speech** -- Listen to reflections with customizable voice and playback rate
- **Auto Voice-Over** -- Automatically play reflections during walk-through mode
- **Walk-Through Mode** -- Automated guided tour with sequential image viewing
- **Reflection History** -- Timeline of all reflections with quick navigation
- **Summarize Trajectory** -- Generate a short narrative summary of how the experience moved through the gallery (phenomenological, non-reductive)
- **Separate LLM Tracking** -- Exported data tracks the LLM used for profile generation separately from the VLM/LLM used for reflections and trajectory summaries

## Images & Copyright

Images displayed in this application are served from [fredericbenard.com](https://www.fredericbenard.com) (bilingual EN/FR galleries and captions).

**Copyright 1990-2026 Frederic Benard. All rights reserved.**

These images are not part of the open-source repository and may not be copied, reused, or redistributed without permission.

## Prerequisites

### Node.js

Vite 7 requires **Node.js 20.19+** (or **22.12+**).

### Local Models (Ollama)

```bash
# Pull the vision model (VLM)
ollama pull llava:7b

# Pull the text model for profile and style generation
ollama pull llama3.1:8b-instruct-q5_K_M

# Start Ollama (usually runs automatically)
ollama serve
```

### Cloud Models (OpenAI, Google Gemini, Anthropic)

This app uses **BYOK (bring your own key)**:

- Users paste API keys in the app UI (**Settings > API Keys**).
- Keys are stored in the browser and forwarded in request headers; the server does not store them.

Get keys from:
- **OpenAI** -- [platform.openai.com/api-keys](https://platform.openai.com/api-keys)
- **Google Gemini** -- [aistudio.google.com/apikey](https://aistudio.google.com/apikey)
- **Anthropic** -- [console.anthropic.com](https://console.anthropic.com/)

## Setup

```bash
npm install
```

## Run

```bash
npm run dev
```

Open [http://localhost:5173](http://localhost:5173).

In dev, `npm run dev` starts:
- **Vite** (frontend)
- **Node/Express** on `http://localhost:8787` (the app API server). Vite proxies `/api/*` to it.

**Language:**

- The app detects locale from the browser and persists your choice in localStorage.
- You can switch **EN/FR** via the footer toggle.

### Deploy to Hugging Face Spaces

The app can be deployed to [Hugging Face Spaces](https://huggingface.co/spaces) as a **Docker** Space. Users bring their own API keys (BYOK) via the in-app Settings; no keys are stored on the server. Ollama is not available on HF (the UI falls back to cloud providers). For step-by-step instructions, see **[docs/deploy-huggingface.md](docs/deploy-huggingface.md)**.

## Usage

### 1. Generate or Load a Viewer Profile

Before selecting a gallery, either generate a new profile or load a saved one.

**Generate** -- Click **"Generate viewer profile"** in the sidebar. This creates:

- **Viewer Profile** (7 dimensions) -- Tolerance for ambiguity, attention style, embodied orientation, interpretive posture, aesthetic conditioning, motivational stance, memory integration tendency
- **Reflection Style** (7 dimensions, independent of profile) -- Lexical register, emotion explicitness, voice stability, sensory modality emphasis, self-reference mode, metaphor density, pacing
- **Initial Internal State** (7 dimensions, same schema as evolving state) -- Dominant mood, underlying tension or ease, energy and engagement, emotional openness, attentional focus, meaning-making pressure, somatic activation
- **Short Descriptions** -- LLM-summarized 1-2 sentence versions of each for user-facing display
- **Label** -- A short 2-5 word summary (e.g. "Anxious, detail-oriented observer") for easy identification

Each generation uses parametric hints: 2-4 dimensions are randomly pinned to specific values from a pool, and the LLM resolves the rest to form a coherent whole. This produces diverse, believable outputs.

**Load** -- Click **"Load saved profile"** to see all available profiles (public and saved). The list is not filtered by LLM provider, so you can load a profile generated by one provider and use a different provider for reflections. Each entry shows the label and profile ID; click one to load it. The profile ID is always visible so you can match it to files in `data/profiles/` and `data/reflections/`.

These settings shape all subsequent reflections.

### 2. Select a Gallery

Choose from the scraped galleries. Gallery sections and names follow the selected locale (EN/FR). Images are loaded from fredericbenard.com.

### 3. Reflect on Images

- **Manual Reflection** -- Click **"Reflect on this image"**
- **Walk-Through Mode** -- Click **"Start walk-through"** for an automated sequence:
  - Reflects on each image in order
  - Auto-advances after voice-over completes (if enabled) or after a short delay
  - Can be paused or stopped at any time

### 4. Listen to Reflections

- Click **"Listen"** to hear a reflection
- Adjust **Rate** (0.5-1.5) and **Voice** (browser-provided voices)
- Enable **"Auto voice-over"** to play reflections automatically

### 5. Navigate History

The **Reflection History** panel shows all reflections in sequence. Click any entry to jump directly to that image.

### 6. Summarize Trajectory

After reflecting on at least one image, use **"Summarize trajectory"** to get a short narrative summary of how the experience moved (e.g. gradual settling, oscillation, depletion). This uses the same text LLM as the reflection provider; the summary appears below the buttons.

## How It Works

### Viewer Profile System (v2)

The v2 architecture uses 4 independent generation stages: **Profile**, **Reflection Style**, **Initial State**, and **Stateful Reflection**. Each of the first three stages defines 7 theoretically grounded dimensions.

**Profile Generation**
Creates a stable perceptual and interpretive disposition -- how the viewer characteristically attends to, processes, and makes meaning from visual art. The 7 profile dimensions are:

1. Tolerance for ambiguity (low to high)
2. Attention style (absorbed/dwelling to scanning/restless)
3. Embodied orientation (somatic to cognitive)
4. Interpretive posture (literal/descriptive to symbolic/associative to autobiographical)
5. Aesthetic conditioning (naive to highly conditioned, with art background)
6. Motivational stance (seeking challenge/novelty to seeking comfort/familiarity)
7. Memory integration tendency (integrative/accumulative to discrete/reset)

**Reflection Style** (independent of profile)
Defines how experience is expressed in language -- the texture, rhythm, and habits of inner speech. The 7 style dimensions are:

1. Lexical register (plain/conversational to literary/poetic)
2. Emotion explicitness (implicit/suggested to explicit/named)
3. Voice stability (steady/composed to fragmented/shifting)
4. Sensory modality emphasis (visual, kinesthetic, auditory, or mixed)
5. Self-reference mode (first-person intimate to observational/impersonal)
6. Metaphor density (spare/literal to rich/figurative)
7. Pacing (terse/compressed to expansive/flowing)

**Initial Internal State**
A momentary snapshot of the viewer's inner condition at the moment they enter the gallery, before encountering any images. Uses the same 7-dimension schema as the evolving state:

1. Dominant mood
2. Underlying tension or ease
3. Energy and engagement
4. Emotional openness
5. Attentional focus
6. Meaning-making pressure
7. Somatic activation

**Parametric variability**
Rather than using fixed hint variants, generation uses parametric hints (ported from the research evaluation pipeline). For each generation, 2-4 of the 7 dimensions are randomly pinned to specific values sampled from a pool, and the LLM resolves the remaining dimensions to form a coherent whole. This is defined in `src/prompts.ts` and shared by both the app and the offline generation script.

**Saved profiles**
When you generate a profile **while running the dev server** (`npm run dev`), it is written to `data/profiles/<uuid>.json`. The file includes: UUID, generation timestamp, locale, LLM used, label, profile text (long + short), reflection style text (long + short), initial state text (long + short), and raw LLM outputs.

**Public profiles**
Pre-generated profiles live in `data/profiles/public/` (EN + FR pairs). These are generated offline using `scripts/generate-v2-profiles.ts` and are always available in the profile list.

**Loading profiles**
The dev server exposes `/api/list-profiles` and `/api/load-profile?id=<uuid>`. The UI uses these to list and load all profiles (public + saved) without filtering by provider.

### Stateful Reflections

Each reflection:

- Incorporates the viewer profile and reflection style
- Uses the initial state for the first image, then carries forward the evolving state
- Evolves gradually unless an image is strongly disruptive
- Outputs structured `[REFLECTION]` and `[STATE]` blocks

Conceptual architecture (showing the first few steps):

- \(t_0\): initial internal state (generated once, before image 1)
- For each image \(i \in \{1,2,3,\dots\}\):
  - **Input**: Profile + Style + State(\(t_{i-1}\)) + Image(\(i\))
  - **Output**: Reflection(\(i\)) + updated State(\(t_i\))

This makes it natural to display the internal state as a timeline: \(t_0, t_1, t_2, t_3\) (after images 1‚Äì3).

**Saved reflection sessions**
When you reflect on images with a profile that was successfully saved **and** the dev server is running, each reflection auto-saves the full session to `data/reflections/<profileId>_<galleryId>_<sessionStartedAt>.json`. One file per (profile, gallery) run is updated in place after every new reflection. The file embeds gallery metadata, profile and reflection style text, all reflections so far, and last internal state.

Locale notes:

- Each reflection includes `generatedAt` (ISO) and may include `locale` (EN/FR).
- Trajectory summaries include `trajectorySummaryLocale` and `trajectorySummaryGeneratedAt`.

**LLM provenance in exports**
Exported data (markdown and JSON) tracks three separate LLM contexts:

- `profileLlm` / `profileLlmModelLabel` -- which LLM generated the profile, style, and initial state
- `reflectionLlm` / `reflectionLlmModelLabel` -- which text LLM ran the reflections
- `reflectionVlm` / `reflectionVlmModelLabel` -- which vision model processed the images

These may all be different if you load a profile generated by one provider and reflect with another.

### Experiential Trajectory Analysis

Reflection sessions can be treated as **experiential trajectories**: ordered paths of internal state and reflection through a gallery, shaped by profile and reflection style. Analysis stays qualitative and phenomenological -- no valence/arousal or sentiment scores.

**In the app**
Use **"Summarize trajectory"** in the Reflection history section (after reflecting on at least one image) to get a narrative summary of the current run.

**Data model**

- `src/lib/trajectory.ts` -- Defines `ExperientialTrajectory` and `trajectoryFromSession(session)`. A trajectory is an ordered sequence of steps (reflection text + internal state per image), plus gallery and viewer context.

**Implemented**

- **Narrative summarization** (`src/lib/analyzeTrajectory.ts`) -- `generateNarrativeSummary(trajectory, initialState, provider, locale)` produces a short reflective summary of how the experience moved (e.g. gradual settling, oscillation, depletion, drift). In the UI this is triggered by the button in the Reflection history section; programmatically, load a session from `data/reflections/*.json`, convert with `trajectoryFromSession()`, then call `generateNarrativeSummary()` with your chosen LLM provider and target locale.

## Research

The `research/` directory contains the evaluation pipeline used to develop and validate the v2 prompt architecture. It includes:

- **Parametric variant generation** (`research/eval_pipeline/parametric.py`) -- the dimension pools and subset-sampling logic that the app's TypeScript parametric hints are ported from
- **Experiments** for profile generation, style generation, initial state generation, and stateful reflection (with high/low ambiguity variants)
- **Evaluation criteria** and **provenance tracking** for each experiment

See `research/docs/README.md` for details.

## Research Positioning

Stateful Viewers draws on reception theory, phenomenology, and aesthetic psychology. It models a viewer's perceptual stance prior to viewing, maintains a stable expressive voice across images, and treats emotional response as something that unfolds over time.

The system operationalizes qualitative theories of aesthetic experience within a structured generative framework, without reducing experience to numerical scores or fixed emotion labels.

For deeper background, see:

- [Art and emotions (lit review)](research/docs/literature/art-and-emotions.md) ‚Äî ambiguity, ‚Äúbeing moved‚Äù, embodied response, temporal unfolding
- [v2 dimensions (design)](research/docs/design/v2-dimensions.md) ‚Äî theoretical grounding for profile √ó style √ó internal state

### Prompt-to-Research Mapping

| Prompt              | Research Anchor                                                | Core Thinkers                  |
| ------------------- | -------------------------------------------------------------- | ------------------------------ |
| Viewer Profile      | Reception theory, phenomenology of perception                  | Jauss, Merleau-Ponty, Gombrich |
| Reflection Style    | Inner speech, narrative psychology, phenomenological reduction | Vygotsky, Bruner, Husserl      |
| Initial State       | Situatedness, pre-reflective experience, embodied entry        | Merleau-Ponty, Husserl, Dewey  |
| Stateful Reflection | Aesthetic experience as process, affect dynamics               | Dewey, Tomkins                 |

### Relation to Affective Computing

While the system tracks internal state over time, it differs fundamentally from affective computing. Rather than detecting, classifying, or predicting emotion, it models aesthetic experience as situated, qualitative, and temporally unfolding.

Emotional state is treated as one component of lived experience, expressed through a stable reflective voice and shaped by prior orientation, attention style, and aesthetic conditioning.

Where affective computing often asks what emotion is present, Stateful Viewers asks *what it is like to encounter this image, having already encountered the previous ones.*

| Dimension         | Affective Computing       | Stateful Viewers          |
| ----------------- | ------------------------- | ------------------------- |
| Goal              | Detect / classify emotion | Simulate lived experience |
| View of the human | Signal source             | Situated subject          |
| Emotion           | Target variable           | Embedded component        |
| Representation    | Numeric / categorical     | Qualitative / narrative   |
| Time              | Discrete steps            | Continuous accumulation   |
| Ambiguity         | Minimized                 | Preserved                 |
| Outcome           | Prediction / adaptation   | Reflection / articulation |

## Project Structure

- `docs/` -- Prompt examples (profile, reflection style, profile label, stateful reflection, trajectory summary) and deployment guide ([deploy-huggingface.md](docs/deploy-huggingface.md))
- `data/profiles/public/` -- Pre-generated v2 public profiles (EN + FR pairs)
- `data/profiles/` -- User-generated profiles (gitignored)
- `data/reflections/` -- Auto-saved reflection sessions (gitignored)
- `research/` -- Evaluation pipeline, experiments, and parametric variant generation
- `scripts/generate-v2-profiles.ts` -- Generate v2 public profiles (EN + FR) offline
- `scripts/scrape-galleries.ts` -- Gallery scraper
- `scripts/add-profile-labels.ts` -- Add labels to legacy profiles missing them
- `src/data/galleries.ts` -- Gallery data types
- `src/data/galleries.en.json` -- Scraped gallery data (EN)
- `src/data/galleries.fr.json` -- Scraped gallery data (FR)
- `src/api/vision.ts` -- Unified vision API router
- `src/api/ollama.ts` -- Ollama/LLaVA-1.6 client
- `src/api/openai.ts` -- OpenAI client
- `src/api/gemini.ts` -- Gemini client
- `src/api/anthropic.ts` -- Anthropic client
- `src/api/llm.ts` -- Text-only LLM interface
- `src/prompts.ts` -- All prompt definitions: profile, style, initial state, label, short descriptions, stateful reflection, parametric hint generation
- `src/lib/parseReflection.ts` -- Parse `[REFLECTION]` / `[STATE]` blocks
- `src/lib/exportSession.ts` -- Export session data (markdown + JSON, with separate LLM provenance)
- `src/lib/saveProfile.ts` -- Save generated profiles to `data/profiles/`
- `src/lib/loadProfile.ts` -- List and load profiles (public + saved, unfiltered)
- `src/lib/saveReflectionSession.ts` -- Auto-save reflection sessions to `data/reflections/`
- `src/lib/trajectory.ts` -- Experiential trajectory types and `trajectoryFromSession()`
- `src/lib/analyzeTrajectory.ts` -- Phenomenological analysis (narrative summary; extensible)
- `src/hooks/useSpeech.ts` -- Text-to-speech utilities
- `src/App.tsx` -- Main UI and state management

## Scripts

**Refresh gallery data** -- To refresh galleries from fredericbenard.com:

```bash
npm run scrape
```

This updates `src/data/galleries.en.json` and `src/data/galleries.fr.json`.

**Generate v2 public profiles** -- To generate (or regenerate) the public profiles:

```bash
npx tsx scripts/generate-v2-profiles.ts
```

Generates 4 EN profiles with parametric hints, translates each to FR, and writes all 8 to `data/profiles/public/`. Requires `OPENAI_API_KEY` in `.env` or environment.

**Add profile labels** -- To add labels to existing profiles that are missing them (e.g. profiles created before the label feature):

```bash
npm run add-labels
```

The script reads API keys from environment variables, detects which providers are available, and only processes profiles for those providers. Profiles that already have a label are skipped. See `scripts/add-profile-labels.ts`.

Common variables:

- `OPENAI_API_KEY`
- `GOOGLE_API_KEY`
- `ANTHROPIC_API_KEY`
- `OLLAMA_BASE_URL` (optional; defaults to `http://localhost:11434`)

## Technical Notes

- **Vision Models**: LLaVA-1.6 7B (Ollama), GPT-5.2, Gemini 3 Pro (preview), Claude Sonnet 4.5
- **Text Models**: Llama 3.1 8B Instruct (Q5_K_M quantized) (Ollama), GPT-5.2, Gemini 3 Pro (preview), Claude Sonnet 4.5
- **API Proxying**: In dev, the frontend calls `/api/*`; Vite proxies most `/api/*` to the local Node/Express server, which proxies cloud providers. `/api/ollama/*` is proxied directly to Ollama for local usage.
- **Image Source**: Images proxied from fredericbenard.com
- **State Management**: React hooks with per-gallery state tracking
- **Text-to-Speech**: Web Speech API
- **LLM Provenance**: The profile-generation LLM is tracked separately from the reflection VLM/LLM. This allows cross-provider workflows (e.g. generate profile with OpenAI, reflect with Gemini) with full traceability in exports.
- **Saved data** (dev + HF server): `data/profiles/` (generated profiles), `data/profiles/public/` (pre-generated public profiles, checked in), `data/reflections/` (reflection sessions); generated profiles and reflections are gitignored. Saving and loading are implemented by the Node/Express server (`/api/save-profile`, `/api/save-reflection-session`, `/api/list-profiles`, `/api/load-profile`). In dev, Vite proxies to this server; on Hugging Face Spaces, the same server runs inside the Docker container.

For production deployment to a static host, a backend is required to proxy cloud model requests and (if you want persistence) to provide save endpoints. **Hugging Face Spaces** is supported: use a Docker Space and follow [docs/deploy-huggingface.md](docs/deploy-huggingface.md); the repo's Dockerfile and frontmatter (`sdk: docker`, `app_port: 7860`) are already configured for HF.

## References

**Phenomenology & Perception**

- Merleau-Ponty, M. (2012). *Phenomenology of perception* (D. A. Landes, Trans.). Routledge. (Original work published 1945)
- Husserl, E. (1982). *Ideas pertaining to a pure phenomenology and to a phenomenological philosophy, First Book* (F. Kersten, Trans.). Springer. (Original work published 1913)

**Reception Theory & Viewer Orientation**

- Jauss, H. R. (1982). *Toward an aesthetic of reception* (T. Bahti, Trans.). University of Minnesota Press.
- Gombrich, E. H. (1960). *Art and illusion: A study in the psychology of pictorial representation*. Princeton University Press.

**Empirical Aesthetics & Aesthetic Emotion**

- Leder, H., Belke, B., Oeberst, A., & Augustin, D. (2004). A model of aesthetic appreciation and aesthetic judgments. *British Journal of Psychology*, 95(4), 489‚Äì508.
- Silvia, P. J. (2005). Emotional responses to art: From collation and arousal to cognition and emotion. *Review of General Psychology*, 9(4), 342‚Äì357.
- Pelowski, M., Markey, P. S., Forster, M., Gerger, G., & Leder, H. (2017). Move me, astonish me‚Ä¶ delight my eyes and brain: The Vienna Integrated Model of top-down and bottom-up processes in Art Perception (VIMAP) and corresponding affective, evaluative, and neurophysiological correlates. *Physics of Life Reviews*, 21, 80‚Äì125.

**Ambiguity, Interest, and Optimal Complexity**

- Berlyne, D. E. (1971). *Aesthetics and psychobiology*. Appleton-Century-Crofts.
- Jakesch, M., & Leder, H. (2009). Finding meaning in art: Preferred levels of ambiguity in art appreciation. *Quarterly Journal of Experimental Psychology*, 62(11), 2105‚Äì2112.

**Aesthetic Experience as Process**

- Dewey, J. (2005). *Art as experience*. Perigee Books. (Original work published 1934)

**Inner Speech, Narrative, and Expression**

- Vygotsky, L. S. (1986). *Thought and language* (A. Kozulin, Trans.). MIT Press. (Original work published 1934)
- Bruner, J. (1990). *Acts of meaning*. Harvard University Press.

**Embodiment, ‚ÄúBeing Moved‚Äù, and Neuroaesthetics**

- Wassiliwizky, E., Wagner, V., Jacobsen, T., & Menninghaus, W. (2015). Art-elicited chills indicate states of being moved. *Psychology of Aesthetics, Creativity, and the Arts*, 9(4), 413‚Äì427.
- Vessel, E. A., Starr, G. G., & Rubin, N. (2012). The brain on art: Intense aesthetic experience activates the default mode network. *Frontiers in Human Neuroscience*, 6, 66.
- Chatterjee, A., & Vartanian, O. (2014). Neuroaesthetics. *Trends in Cognitive Sciences*, 18(7), 370‚Äì375.

**Affect Theory & Affective Computing (for contrast)**

- Picard, R. W. (1997). *Affective computing*. MIT Press.
- Russell, J. A. (1980). A circumplex model of affect. *Journal of Personality and Social Psychology*, 39(6), 1161-1178.
- Tomkins, S. S. (1962). *Affect, imagery, consciousness: Vol. 1. The positive affects*. Springer.
